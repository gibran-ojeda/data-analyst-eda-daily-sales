{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "581b76d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import os\n",
    "import fnmatch\n",
    "from datetime import datetime\n",
    "from typing import List, Optional\n",
    "import pyarrow as pa\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "5af65137",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column name constants\n",
    "WAREHOUSE = \"Almacen\"\n",
    "DATE = \"Fecha\"\n",
    "CUSTOMER = \"Cliente\"\n",
    "SELLER = \"Vendedor\"\n",
    "PRODUCT_CONCAT = \"ProdConcat\"\n",
    "QUANTITY = \"Cantidad\"\n",
    "SALE_PRICE = \"PrecioVenta\"\n",
    "PAYMENT_METHODS = \"Metodos De Pago\"\n",
    "NO_MOV = \"NoMov\"\n",
    "\n",
    "DIRECTORY = \"../data/sales\"\n",
    "KEYWORDS = \"Ventas por Tickets\"\n",
    "SALES_COLUMNS =  [\n",
    "    WAREHOUSE,\n",
    "    NO_MOV,\n",
    "    DATE,\n",
    "    CUSTOMER,\n",
    "    SELLER,\n",
    "    PRODUCT_CONCAT,\n",
    "    QUANTITY,\n",
    "    SALE_PRICE,\n",
    "    PAYMENT_METHODS\n",
    "]\n",
    "OUTPUT_PATH = \"../output\"\n",
    "OUTPUT_FILE_NAME = \"salesReportMerged\"\n",
    "# EDA Metric constants\n",
    "TOTAL_SALES = \"TotalSales\"\n",
    "SALES_MEAN = \"PROMEDIO\"\n",
    "SALES_MEDIAN = \"MEDIANA\"\n",
    "SALES_STDDEV = \"DESVIACION ESTANDAR\"\n",
    "SALES_MIN = \"VENTA MINIMA\"\n",
    "SALES_MAX = \"VENTA MAXIMA\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "803228b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readExcelToDataFrame(path: str, sheetName: Optional[str] = None) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Reads an Excel (.xlsx) file and returns it as a Polars DataFrame.\n",
    "\n",
    "    :param path: Full path to the Excel file\n",
    "    :param sheetName: Name of the sheet to read (default is the first sheet)\n",
    "    :return: A pl.DataFrame object\n",
    "\n",
    "    Example usage:\n",
    "    >>> df = readExcelToDataFrame(\"./data/salesReport.xlsx\")\n",
    "    >>> print(df)\n",
    "\n",
    "    >>> dfSheet = readExcelToDataFrame(\"./data/salesReport.xlsx\", sheetName=\"JanuarySales\")\n",
    "    >>> print(dfSheet)\n",
    "    \"\"\"\n",
    "    if not path.endswith('.xlsx'):\n",
    "        raise ValueError(\"The file must have a .xlsx extension.\")\n",
    "    \n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"The file '{path}' does not exist.\")\n",
    "    \n",
    "    try:\n",
    "        # Try using Polars native read_excel\n",
    "        dfPolars = pl.read_excel(path, sheet_name=sheetName)\n",
    "    except (AttributeError, TypeError):\n",
    "        # Fallback if Polars version does not support read_excel or sheet_name argument\n",
    "        dfPandas = pd.read_excel(path, sheet_name=sheetName)\n",
    "        dfPolars = pl.from_pandas(dfPandas)\n",
    "\n",
    "    return dfPolars\n",
    "\n",
    "def unionDataFrames(dfList: List[pl.DataFrame]) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Merges a list of Polars DataFrames into a single DataFrame.\n",
    "\n",
    "    :param dfList: List of pl.DataFrame objects\n",
    "    :return: A single pl.DataFrame resulting from concatenating all the DataFrames\n",
    "\n",
    "    Example usage:\n",
    "    >>> df1 = pl.DataFrame({\"a\": [1, 2], \"b\": [3, 4]})\n",
    "    >>> df2 = pl.DataFrame({\"a\": [5, 6], \"b\": [7, 8]})\n",
    "    >>> result = unionDataFrames([df1, df2])\n",
    "    >>> print(result)\n",
    "    \"\"\"\n",
    "    if not dfList:\n",
    "        raise ValueError(\"The DataFrame list is empty.\")\n",
    "    \n",
    "    if not all(isinstance(df, pl.DataFrame) for df in dfList):\n",
    "        raise TypeError(\"All elements in the list must be of type pl.DataFrame.\")\n",
    "    \n",
    "    return pl.concat(dfList)\n",
    "\n",
    "def saveDataFrameToExcel(df: pl.DataFrame, path: str, fileName: str) -> None:\n",
    "    \"\"\"\n",
    "    Converts a Polars DataFrame to Excel and saves it to the specified path.\n",
    "\n",
    "    :param df: A pl.DataFrame object\n",
    "    :param path: Directory path where the Excel file will be saved\n",
    "    :param fileName: Output filename without extension\n",
    "\n",
    "    Example usage:\n",
    "    >>> df = pl.DataFrame({\"a\": [1, 2], \"b\": [3, 4]})\n",
    "    >>> saveDataFrameToExcel(df, \"./outputs\", \"myReport\")\n",
    "    \"\"\"\n",
    "    if not isinstance(df, pl.DataFrame):\n",
    "        raise TypeError(\"The 'df' parameter must be a pl.DataFrame.\")\n",
    "\n",
    "    if not fileName.endswith('.xlsx'):\n",
    "        fileName += '.xlsx'\n",
    "\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "    # Step 1: Try converting normally\n",
    "    try:\n",
    "        dfPandas = df.to_pandas()\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Direct to_pandas() failed. Trying manual conversion. Error: {e}\")\n",
    "        # Step 2: Manual fallback: recreate a Pandas DataFrame manually\n",
    "        dfPandas = pd.DataFrame({col: df[col].to_list() for col in df.columns})\n",
    "\n",
    "    # Step 3: Save as Excel\n",
    "    outputPath = os.path.join(path, fileName)\n",
    "    dfPandas.to_excel(outputPath, index=False)\n",
    "\n",
    "def selectColumns(df: pl.DataFrame, columnsToKeep: List[str]) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Selects specific columns from a Polars DataFrame and returns a new DataFrame.\n",
    "\n",
    "    :param df: A pl.DataFrame object\n",
    "    :param columnsToKeep: List of column names to keep\n",
    "    :return: A pl.DataFrame with only the specified columns\n",
    "\n",
    "    Example usage:\n",
    "    >>> df = pl.DataFrame({\n",
    "    ...     \"name\": [\"Alice\", \"Bob\", \"Charlie\"],\n",
    "    ...     \"age\": [25, 30, 35],\n",
    "    ...     \"city\": [\"NYC\", \"LA\", \"Chicago\"]\n",
    "    ... })\n",
    "    >>> selectedDf = selectColumns(df, [\"name\", \"city\"])\n",
    "    >>> print(selectedDf)\n",
    "    \"\"\"\n",
    "    if not isinstance(df, pl.DataFrame):\n",
    "        raise TypeError(\"The 'df' parameter must be a pl.DataFrame.\")\n",
    "    \n",
    "    if not all(isinstance(col, str) for col in columnsToKeep):\n",
    "        raise TypeError(\"All elements in 'columnsToKeep' must be strings.\")\n",
    "    \n",
    "    missingColumns = [col for col in columnsToKeep if col not in df.columns]\n",
    "    if missingColumns:\n",
    "        raise ValueError(f\"The following columns are not present in the DataFrame: {missingColumns}\")\n",
    "    \n",
    "    return df.select(columnsToKeep)\n",
    "\n",
    "def replaceNaNWithZero(df: pl.DataFrame) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Replaces NaN or null values with 0 in all numeric columns of a Polars DataFrame.\n",
    "\n",
    "    :param df: A pl.DataFrame object to process\n",
    "    :return: A new pl.DataFrame with NaN/null values replaced by 0 in numeric columns only\n",
    "\n",
    "    Example usage:\n",
    "    >>> df = pl.DataFrame({\n",
    "    ...     \"sales\": [100, None, 300],\n",
    "    ...     \"profit\": [50, 75, None],\n",
    "    ...     \"city\": [\"NYC\", \"LA\", \"Chicago\"]\n",
    "    ... })\n",
    "    >>> updatedDf = replaceNaNWithZero(df)\n",
    "    >>> print(updatedDf)\n",
    "    \"\"\"\n",
    "    if not isinstance(df, pl.DataFrame):\n",
    "        raise TypeError(\"The 'df' parameter must be a pl.DataFrame.\")\n",
    "\n",
    "    numericColumns = [col for col, dtype in zip(df.columns, df.dtypes) if dtype in (pl.Int8, pl.Int16, pl.Int32, pl.Int64, pl.UInt8, pl.UInt16, pl.UInt32, pl.UInt64, pl.Float32, pl.Float64)]\n",
    "    \n",
    "    df = df.with_columns(\n",
    "        [pl.col(col).fill_null(0).alias(col) for col in numericColumns]\n",
    "    )\n",
    "    \n",
    "    return df\n",
    "\n",
    "def filterLastNDaysFromMaxDate(df: pl.DataFrame, days: int) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Filters the DataFrame to keep only the last N days from the maximum registered date in the DATE column.\n",
    "\n",
    "    :param df: A Polars DataFrame\n",
    "    :param days: Number of days to go back from the latest date\n",
    "    :return: A filtered Polars DataFrame\n",
    "    \"\"\"\n",
    "    from datetime import timedelta\n",
    "\n",
    "    if not isinstance(df, pl.DataFrame):\n",
    "        raise TypeError(\"The 'df' parameter must be a Polars DataFrame.\")\n",
    "\n",
    "    if DATE not in df.columns:\n",
    "        raise ValueError(f\"The column '{DATE}' does not exist in the DataFrame.\")\n",
    "\n",
    "    # 1. Find the maximum date in the dataset\n",
    "    maxDate = df.select(pl.col(DATE).max()).item()\n",
    "\n",
    "    if not isinstance(maxDate, datetime):\n",
    "        raise ValueError(f\"The maximum value in column '{DATE}' is not a datetime.\")\n",
    "\n",
    "    # 2. Calculate cutoff date\n",
    "    cutoffDate = maxDate - timedelta(days=days)\n",
    "\n",
    "    # 3. Filter the DataFrame\n",
    "    filteredDf = df.filter(pl.col(DATE) >= cutoffDate)\n",
    "\n",
    "    return filteredDf\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "0fa049f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def findExcelFilesByMatch(directory: str, keyword: str, extension: str = \"xlsx\") -> List[str]:\n",
    "    \"\"\"\n",
    "    Finds all files in a directory that contain a specific keyword and have the specified extension.\n",
    "\n",
    "    :param directory: Directory path to search for files\n",
    "    :param keyword: Keyword to match in filenames\n",
    "    :param extension: File extension to match (default is 'xlsx')\n",
    "    :return: List of matching filenames\n",
    "\n",
    "    Example usage:\n",
    "    >>> files = findExcelFilesByMatch(\"./data\", \"sales\")\n",
    "    >>> print(files)\n",
    "\n",
    "    >>> csvFiles = findExcelFilesByMatch(\"./data\", \"sales\", \"csv\")\n",
    "    >>> print(csvFiles)\n",
    "    \"\"\"\n",
    "    matchedFiles = []\n",
    "    pattern = f\"*{keyword}*.{extension}\"\n",
    "    \n",
    "    for fileName in os.listdir(directory):\n",
    "        if fnmatch.fnmatch(fileName, pattern):\n",
    "            matchedFiles.append(fileName)\n",
    "\n",
    "    return matchedFiles\n",
    "\n",
    "def getCurrentTimestamp() -> str:\n",
    "    \"\"\"\n",
    "    Returns the current date and time formatted as 'YYYY-MM-DDTHH-MM-SS'.\n",
    "\n",
    "    :return: A string representing the current timestamp\n",
    "    Example usage:\n",
    "    >>> timestamp = getCurrentTimestamp()\n",
    "    >>> print(timestamp)\n",
    "    \"\"\"\n",
    "    return datetime.now().strftime(\"%Y-%m-%dT%H-%M-%S\")\n",
    "\n",
    "def createFolder(baseFolderName: str, basePath: str = \".\", addTimestamp: bool = True) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Creates a folder with the base name, optionally followed by the current timestamp.\n",
    "\n",
    "    :param baseFolderName: Base name for the folder\n",
    "    :param basePath: Directory where the folder will be created (default is the current directory)\n",
    "    :param addTimestamp: Whether to append the current timestamp to the folder name (default is True)\n",
    "    :return: Full path of the created folder or None if creation fails\n",
    "\n",
    "    Example usage:\n",
    "    >>> folderPath = createFolder(\"report\")\n",
    "    >>> print(folderPath)\n",
    "\n",
    "    >>> folderPathNoTimestamp = createFolder(\"report\", addTimestamp=False)\n",
    "    >>> print(folderPathNoTimestamp)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        fullFolderName = baseFolderName\n",
    "        if addTimestamp:\n",
    "            fullFolderName += f\"_{getCurrentTimestamp()}\"\n",
    "        \n",
    "        fullPath = os.path.join(basePath, fullFolderName)\n",
    "\n",
    "        os.makedirs(fullPath, exist_ok=True)\n",
    "        print(f\"Folder created: {fullPath}\")\n",
    "        return fullPath\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating the folder: {e}\")\n",
    "        return None\n",
    "\n",
    "def moveFilesToFolder(fileList: List[str], destinationFolder: str) -> None:\n",
    "    \"\"\"\n",
    "    Moves a list of files to a destination folder.\n",
    "\n",
    "    :param fileList: List of file paths to move\n",
    "    :param destinationFolder: Path of the destination folder\n",
    "    :return: None\n",
    "\n",
    "    Example usage:\n",
    "    >>> moveFilesToFolder([\"./file1.txt\", \"./file2.txt\"], \"./backup\")\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create the destination folder if it doesn't exist\n",
    "        if not os.path.exists(destinationFolder):\n",
    "            createdFolder = createFolder(destinationFolder, basePath=\".\", addTimestamp=False)\n",
    "            if createdFolder:\n",
    "                destinationFolder = createdFolder\n",
    "            else:\n",
    "                raise Exception(f\"Failed to create the destination folder: {destinationFolder}\")\n",
    "\n",
    "        for filePath in fileList:\n",
    "            if os.path.isfile(filePath):\n",
    "                destinationPath = os.path.join(destinationFolder, os.path.basename(filePath))\n",
    "                shutil.move(filePath, destinationPath)\n",
    "                print(f\"File moved: {filePath} -> {destinationPath}\")\n",
    "            else:\n",
    "                print(f\"File does not exist: {filePath}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error moving files: {e}\")\n",
    "\n",
    "def validateFiles(fileList: List[str]) -> bool:\n",
    "    \"\"\"\n",
    "    Validates that a list of file paths does not contain empty values.\n",
    "\n",
    "    :param fileList: List of file paths\n",
    "    :return: True if all files are valid, False if there are missing files\n",
    "\n",
    "    Example usage:\n",
    "    >>> isValid = validateFiles([\"file1.xlsx\", \"file2.xlsx\", \"\"])\n",
    "    >>> print(isValid)\n",
    "    \"\"\"\n",
    "    missingFiles = [filePath for filePath in fileList if filePath.strip() == \"\"]\n",
    "\n",
    "    if missingFiles:\n",
    "        print(\"Error: Missing required files for the report process.\")\n",
    "        return False\n",
    "\n",
    "    print(\"All files are valid.\")\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "1f9b940f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createMergedDataFrameFromExcelMatch(\n",
    "    directory: str = \"./\",\n",
    "    keyword: str = \"\",\n",
    "    columns: Optional[List[str]] = None,\n",
    "    sheetName: Optional[str] = None,\n",
    "    saveToExcel: bool = False,\n",
    "    outputPath: Optional[str] = None,\n",
    "    outputFileName: str = \"mergedReport\"\n",
    ") -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Searches for Excel files in a directory that match a keyword,\n",
    "    reads them into DataFrames (optionally filtering columns and selecting a specific sheet),\n",
    "    merges them into a single DataFrame, and optionally saves it as an Excel file.\n",
    "\n",
    "    :param directory: Directory path to search for Excel files (default \"./\")\n",
    "    :param keyword: Keyword to match in filenames\n",
    "    :param columns: List of columns to select from each file (optional)\n",
    "    :param sheetName: Name of the sheet to read (optional)\n",
    "    :param saveToExcel: Whether to save the merged DataFrame to an Excel file (default False)\n",
    "    :param outputPath: Directory where the Excel file will be saved (required if saveToExcel=True)\n",
    "    :param outputFileName: Name of the output Excel file without extension (default \"mergedReport\")\n",
    "    :return: A single merged pl.DataFrame from all successfully read files\n",
    "    :raises FileNotFoundError: If no matching files are found\n",
    "    :raises ValueError: If no DataFrames could be read or merged\n",
    "     \n",
    "    Example usage:\n",
    "    >>> df = createMergedDataFrameFromExcelMatch(\"./data\", \"sales\", [\"id\", \"amount\"], \"January\", True, \"./outputs\", \"salesJanuary\")\n",
    "    >>> print(df)\n",
    "    \"\"\"\n",
    "    # 1. Find matching Excel files\n",
    "    matchingFiles = findExcelFilesByMatch(directory, keyword, extension=\"xlsx\")\n",
    "    \n",
    "    if not matchingFiles:\n",
    "        raise FileNotFoundError(f\"No Excel files matching '{keyword}' were found in '{directory}'.\")\n",
    "\n",
    "    # 2. Read all files into DataFrames\n",
    "    dataFrames = []\n",
    "    for fileName in matchingFiles:\n",
    "        fullPath = os.path.join(directory, fileName)\n",
    "        try:\n",
    "            df = readExcelToDataFrame(fullPath, sheetName=sheetName)\n",
    "            if columns:\n",
    "                df = selectColumns(df, columns)\n",
    "            dataFrames.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not process file '{fileName}': {e}\")\n",
    "\n",
    "    # 3. Validate at least one DataFrame was read\n",
    "    if not dataFrames:\n",
    "        raise ValueError(\"No valid DataFrames could be created from the matching files.\")\n",
    "\n",
    "    # 4. Merge all DataFrames\n",
    "    mergedDataFrame = unionDataFrames(dataFrames)\n",
    "\n",
    "    # 5. Save to Excel if requested\n",
    "    if saveToExcel:\n",
    "        if outputPath is None:\n",
    "            raise ValueError(\"outputPath must be provided when saveToExcel=True.\")\n",
    "        saveDataFrameToExcel(mergedDataFrame, outputPath, outputFileName)\n",
    "\n",
    "    return mergedDataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "173af814",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanDateColumn(df: pl.DataFrame, dateColumnName: str) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Converts the specified column in a DataFrame from string format to a proper Datetime type.\n",
    "\n",
    "    :param df: A pl.DataFrame containing the date column\n",
    "    :param dateColumnName: Name of the column to convert\n",
    "    :return: A pl.DataFrame with the specified column converted to Datetime type\n",
    "     \n",
    "    Example usage:\n",
    "    >>> dfCleaned = cleanDateColumn(dfSalesMerged, DATE)\n",
    "    >>> print(dfCleaned)\n",
    "    \"\"\"\n",
    "    if not isinstance(df, pl.DataFrame):\n",
    "        raise TypeError(\"The 'df' parameter must be a pl.DataFrame.\")\n",
    "\n",
    "    if dateColumnName not in df.columns:\n",
    "        raise ValueError(f\"The column '{dateColumnName}' does not exist in the DataFrame.\")\n",
    "\n",
    "    return df.with_columns([\n",
    "        pl.col(dateColumnName)\n",
    "        .str.strptime(pl.Datetime, format=\"%b %e %Y %I:%M%p\")\n",
    "        .alias(dateColumnName)\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd52e41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveFigureToPdf(fig, pdf: PdfPages) -> None:\n",
    "    \"\"\"\n",
    "    Saves a matplotlib figure into a PDF file with a white background.\n",
    "\n",
    "    :param fig: A matplotlib figure object\n",
    "    :param pdf: A PdfPages object to save the figure\n",
    "    \"\"\"\n",
    "    pdf.savefig(fig, facecolor='white', edgecolor='white')\n",
    "    plt.close(fig)\n",
    "\n",
    "def prepareSummaryStatistics(df: pl.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Prepares summary statistics for the sales prices grouped by warehouse and ticket (Warehouse + NoMov):\n",
    "    - Mean\n",
    "    - Median\n",
    "    - Standard Deviation\n",
    "    - Minimum\n",
    "    - Maximum\n",
    "    All values are rounded to 2 decimal places.\n",
    "\n",
    "    :param df: A Polars DataFrame\n",
    "    :return: A Pandas DataFrame with the summary statistics\n",
    "    \"\"\"\n",
    "    # 1. Aggregate total sale per warehouse and ticket (Warehouse + NoMov)\n",
    "    ticketSales = df.group_by([WAREHOUSE, NO_MOV]).agg([\n",
    "        pl.col(SALE_PRICE).sum().alias(TOTAL_SALES)\n",
    "    ])\n",
    "\n",
    "    # 2. Calculate summary statistics over the ticket totals\n",
    "    summary = ticketSales.select([\n",
    "        pl.col(TOTAL_SALES).mean().alias(SALES_MEAN),\n",
    "        pl.col(TOTAL_SALES).median().alias(SALES_MEDIAN),\n",
    "        pl.col(TOTAL_SALES).std().alias(SALES_STDDEV),\n",
    "        pl.col(TOTAL_SALES).min().alias(SALES_MIN),\n",
    "        pl.col(TOTAL_SALES).max().alias(SALES_MAX)\n",
    "    ])\n",
    "    \n",
    "    # 3. Convert manually to Pandas DataFrame\n",
    "    summaryDf = pd.DataFrame(summary.to_dict(as_series=False))\n",
    "    \n",
    "    # 4. Round all numeric values to 2 decimal places\n",
    "    summaryDf = summaryDf.round(2)\n",
    "    \n",
    "    return summaryDf\n",
    "\n",
    "def prepareTopSellers(\n",
    "    df: pl.DataFrame,\n",
    "    limit: int = 5,\n",
    "    ascending: bool = False,\n",
    "    days: int = 30\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Prepares the top or bottom sellers by total sales within the last 'days' from the latest recorded date.\n",
    "\n",
    "    :param df: A Polars DataFrame containing sales data\n",
    "    :param limit: Number of sellers to return (default 5)\n",
    "    :param ascending: Sort in ascending order for bottom sellers, or descending for top sellers (default False)\n",
    "    :param days: Number of days to look back from the latest date to filter sales data (default 30)\n",
    "    :return: A Pandas DataFrame with the top or bottom sellers\n",
    "    \"\"\"\n",
    "    # 1. Filter the DataFrame to keep only the last 'days' from the maximum date in the dataset\n",
    "    df = filterLastNDaysFromMaxDate(df, days=days)\n",
    "\n",
    "    # 2. Group by seller and calculate total sales\n",
    "    topSellers = df.group_by(SELLER).agg([\n",
    "        pl.col(SALE_PRICE).sum().alias(TOTAL_SALES)\n",
    "    ]).sort(TOTAL_SALES, descending=not ascending)\n",
    "\n",
    "    # 3. Take only the top or bottom 'limit' sellers\n",
    "    topSellers = topSellers.head(limit)\n",
    "\n",
    "    # 4. Manual conversion to Pandas DataFrame\n",
    "    return pd.DataFrame(topSellers.to_dict(as_series=False))\n",
    "\n",
    "def prepareSalesByDay(df: pl.DataFrame, days: int = 30) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Aggregates total sales per day, limited to the most recent N days.\n",
    "\n",
    "    :param df: A Polars DataFrame containing sales data\n",
    "    :param days: Number of most recent days to include (default is 30)\n",
    "    :return: A Pandas DataFrame with total sales aggregated by day\n",
    "    \"\"\"\n",
    "    # 1. Filter the DataFrame to include only the last 'days' from the most recent date\n",
    "    df = filterLastNDaysFromMaxDate(df, days=days)\n",
    "\n",
    "    # 2. Group sales by date and calculate total sales per day\n",
    "    salesByDay = df.group_by(pl.col(DATE).dt.date()).agg([\n",
    "        pl.col(SALE_PRICE).sum().alias(TOTAL_SALES)\n",
    "    ]).sort(DATE)\n",
    "\n",
    "    # 3. Convert manually to Pandas DataFrame\n",
    "    return pd.DataFrame(salesByDay.to_dict(as_series=False))\n",
    "\n",
    "def prepareSalesDistributions(df: pl.DataFrame, days: int = 30) -> None:\n",
    "    \"\"\"\n",
    "    Genera histogramas y diagramas de caja (boxplots) para los precios de venta,\n",
    "    tanto en escala original como en escala logarítmica.\n",
    "    Filtra los datos para incluir solo los últimos N días a partir de la fecha más reciente.\n",
    "    Muestra etiquetas en valores reales, incluso para los gráficos logarítmicos.\n",
    "\n",
    "    :param df: Un DataFrame de Polars\n",
    "    :param days: Número de días recientes a considerar (por defecto 30)\n",
    "    :yield: Figuras de Matplotlib listas para ser guardadas en PDF\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "\n",
    "    # 1. Filtrar últimos N días\n",
    "    df = filterLastNDaysFromMaxDate(df, days=days)\n",
    "\n",
    "    # 2. Convertir a pandas\n",
    "    salesData = pd.DataFrame(df.select(SALE_PRICE).to_dict(as_series=False))\n",
    "\n",
    "    # 3. Histograma - Escala logarítmica con etiquetas reales\n",
    "    salesDataLog = salesData.copy()\n",
    "    salesDataLog[SALE_PRICE] = salesDataLog[SALE_PRICE].apply(lambda x: np.log1p(x))\n",
    "\n",
    "    fig2, ax2 = plt.subplots(facecolor=\"white\")\n",
    "    salesDataLog.hist(bins=50, ax=ax2, color=\"salmon\", edgecolor=\"black\")\n",
    "    ax2.set_title(f\"Histograma (últimos {days} días)\", fontsize=16)\n",
    "    ax2.set_xlabel(\"Precio de Venta\")\n",
    "    ax2.set_ylabel(\"Frecuencia\")\n",
    "\n",
    "    # Etiquetas del eje X transformadas\n",
    "    ticks = [0, 2, 4, 6, 8, 10]\n",
    "    labels = [f\"${int(np.expm1(t)):,}\" for t in ticks]\n",
    "    ax2.set_xticks(ticks)\n",
    "    ax2.set_xticklabels(labels)\n",
    "\n",
    "    plt.grid(True, linestyle=\"--\", linewidth=0.5)\n",
    "    plt.tight_layout()\n",
    "    yield fig2\n",
    "\n",
    "    # 4. Boxplot - Escala logarítmica con etiquetas reales\n",
    "    fig4, ax4 = plt.subplots(facecolor=\"white\")\n",
    "    salesDataLog.boxplot(ax=ax4)\n",
    "    ax4.set_title(f\"Boxplot (últimos {days} días)\", fontsize=16)\n",
    "    ax4.set_ylabel(\"Precio de Venta\")\n",
    "\n",
    "    y_ticks = [0, 2, 4, 6, 8, 10]\n",
    "    y_labels = [f\"${int(np.expm1(y)):,}\" for y in y_ticks]\n",
    "    ax4.set_yticks(y_ticks)\n",
    "    ax4.set_yticklabels(y_labels)\n",
    "\n",
    "    plt.grid(True, linestyle=\"--\", linewidth=0.5)\n",
    "    plt.tight_layout()\n",
    "    yield fig4\n",
    "\n",
    "def generateSalesEDA(df: pl.DataFrame, outputPdfPath: str, days: int = 30) -> None:\n",
    "    \"\"\"\n",
    "    Generates the Exploratory Data Analysis (EDA) for sales data and saves it to a PDF.\n",
    "\n",
    "    :param df: A Polars DataFrame\n",
    "    :param outputPdfPath: Path where the PDF will be saved\n",
    "    \"\"\"\n",
    "    os.makedirs(os.path.dirname(outputPdfPath), exist_ok=True)\n",
    "    pdf = PdfPages(outputPdfPath)\n",
    "    \n",
    "    # 1. Estadísticas Resumen\n",
    "    summaryStats = prepareSummaryStatistics(df)\n",
    "    summaryDict = summaryStats.to_dict(orient=\"records\")[0]\n",
    "    fig, ax = plt.subplots(figsize=(8.5, 11), facecolor=\"white\")\n",
    "    # Datos\n",
    "    labels = list(summaryDict.keys())\n",
    "    values = list(summaryDict.values())\n",
    "    # Barras horizontales\n",
    "    bars = ax.barh(labels, values, color=\"skyblue\", edgecolor=\"black\")\n",
    "    # Escala log en X\n",
    "    ax.set_xscale(\"log\")\n",
    "    # Etiquetas dentro de cada barra (valores reales con formato bonito)\n",
    "    for bar, value in zip(bars, values):\n",
    "        ax.text(value, bar.get_y() + bar.get_height() / 2,\n",
    "                f\"${int(value):,}\", va=\"center\", ha=\"left\", fontsize=9, color=\"black\")\n",
    "    # Personalización\n",
    "    plt.title(\"Estadísticas Resumen de Precios de Venta\", fontsize=20)\n",
    "    plt.xlabel(\"Valor\", fontsize=14)\n",
    "    # Etiquetas del eje X en formato real (aunque esté log)\n",
    "    ticks = [1, 10, 100, 1_000, 10_000, 100_000]\n",
    "    labels = [f\"${tick:,}\" for tick in ticks]\n",
    "    ax.set_xticks(ticks)\n",
    "    ax.set_xticklabels(labels)\n",
    "\n",
    "    plt.grid(True, axis='x', linestyle=\"--\", linewidth=0.5)\n",
    "    plt.yticks(rotation=45, fontsize=6, ha=\"right\", rotation_mode=\"anchor\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Guardar al PDF\n",
    "    saveFigureToPdf(fig, pdf)\n",
    "\n",
    "    # 2. Ventas por día\n",
    "    salesByDay = prepareSalesByDay(df, days=days)\n",
    "    fig, ax = plt.subplots(facecolor=\"white\")\n",
    "    salesByDay.plot(x=DATE, y=TOTAL_SALES, kind=\"line\", ax=ax)\n",
    "    plt.title(f\"Ventas Totales por Día (últimos {days} días)\", fontsize=16)\n",
    "    plt.xlabel(\"Fecha\", fontsize=10)\n",
    "    plt.ylabel(\"Ventas Totales\", fontsize=12)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid()\n",
    "    plt.tight_layout()\n",
    "    saveFigureToPdf(fig, pdf)\n",
    "\n",
    "    # 3. Mejores vendedores\n",
    "    topBestSellers = prepareTopSellers(df, limit=20, ascending=False, days=days)\n",
    "    fig, ax = plt.subplots(figsize=(8.5, 11), facecolor=\"white\")\n",
    "    ax.set_facecolor(\"white\")\n",
    "    topBestSellers.plot(x=SELLER, y=TOTAL_SALES, kind=\"bar\", ax=ax, color=\"skyblue\", edgecolor=\"black\")\n",
    "    plt.title(f\"Top 20 Mejores Vendedores (últimos {days} días)\", fontsize=20)\n",
    "    plt.xlabel(\"Vendedor\", fontsize=8)\n",
    "    plt.ylabel(\"Ventas Totales\", fontsize=14)\n",
    "    plt.xticks(rotation=45, fontsize=6, ha=\"right\", rotation_mode=\"anchor\")\n",
    "    plt.yticks(fontsize=12)\n",
    "    plt.grid(True, color=\"grey\", linestyle=\"--\", linewidth=0.5)\n",
    "    plt.tight_layout()\n",
    "    saveFigureToPdf(fig, pdf)\n",
    "\n",
    "    # 4. Peores vendedores\n",
    "    topWorstSellers = prepareTopSellers(df, limit=20, ascending=True, days=days)\n",
    "    fig, ax = plt.subplots(figsize=(8.5, 11), facecolor=\"white\")\n",
    "    ax.set_facecolor(\"white\")\n",
    "    topWorstSellers.plot(x=SELLER, y=TOTAL_SALES, kind=\"bar\", ax=ax, color=\"skyblue\", edgecolor=\"black\")\n",
    "    plt.title(f\"Top 20 Peores Vendedores (últimos {days} días)\", fontsize=20)\n",
    "    plt.xlabel(\"Vendedor\", fontsize=8)\n",
    "    plt.ylabel(\"Ventas Totales\", fontsize=14)\n",
    "    plt.xticks(rotation=45, fontsize=6, ha=\"right\", rotation_mode=\"anchor\")\n",
    "    plt.yticks(fontsize=12)\n",
    "    plt.grid(True, color=\"grey\", linestyle=\"--\", linewidth=0.5)\n",
    "    plt.tight_layout()\n",
    "    saveFigureToPdf(fig, pdf)\n",
    "\n",
    "    # 4. Sales distributions\n",
    "    for fig in prepareSalesDistributions(df, days=days):\n",
    "        saveFigureToPdf(fig, pdf)\n",
    "\n",
    "    # Close the PDF\n",
    "    pdf.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d752261",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not determine dtype for column 1, falling back to string\n",
      "Could not determine dtype for column 2, falling back to string\n",
      "Could not determine dtype for column 3, falling back to string\n",
      "Could not determine dtype for column 10, falling back to string\n",
      "Could not determine dtype for column 12, falling back to string\n",
      "Could not determine dtype for column 13, falling back to string\n",
      "Could not determine dtype for column 20, falling back to string\n",
      "Could not determine dtype for column 21, falling back to string\n",
      "Could not determine dtype for column 28, falling back to string\n",
      "Could not determine dtype for column 29, falling back to string\n",
      "Could not determine dtype for column 30, falling back to string\n",
      "Could not determine dtype for column 31, falling back to string\n",
      "Could not determine dtype for column 32, falling back to string\n",
      "Could not determine dtype for column 33, falling back to string\n",
      "Could not determine dtype for column 37, falling back to string\n",
      "Could not determine dtype for column 38, falling back to string\n",
      "Could not determine dtype for column 39, falling back to string\n",
      "Could not determine dtype for column 40, falling back to string\n",
      "Could not determine dtype for column 41, falling back to string\n",
      "Could not determine dtype for column 42, falling back to string\n",
      "Could not determine dtype for column 1, falling back to string\n",
      "Could not determine dtype for column 2, falling back to string\n",
      "Could not determine dtype for column 3, falling back to string\n",
      "Could not determine dtype for column 10, falling back to string\n",
      "Could not determine dtype for column 12, falling back to string\n",
      "Could not determine dtype for column 13, falling back to string\n",
      "Could not determine dtype for column 20, falling back to string\n",
      "Could not determine dtype for column 21, falling back to string\n",
      "Could not determine dtype for column 28, falling back to string\n",
      "Could not determine dtype for column 29, falling back to string\n",
      "Could not determine dtype for column 30, falling back to string\n",
      "Could not determine dtype for column 31, falling back to string\n",
      "Could not determine dtype for column 32, falling back to string\n",
      "Could not determine dtype for column 33, falling back to string\n",
      "Could not determine dtype for column 37, falling back to string\n",
      "Could not determine dtype for column 38, falling back to string\n",
      "Could not determine dtype for column 39, falling back to string\n",
      "Could not determine dtype for column 40, falling back to string\n",
      "Could not determine dtype for column 41, falling back to string\n",
      "Could not determine dtype for column 42, falling back to string\n",
      "Could not determine dtype for column 1, falling back to string\n",
      "Could not determine dtype for column 2, falling back to string\n",
      "Could not determine dtype for column 3, falling back to string\n",
      "Could not determine dtype for column 10, falling back to string\n",
      "Could not determine dtype for column 12, falling back to string\n",
      "Could not determine dtype for column 13, falling back to string\n",
      "Could not determine dtype for column 20, falling back to string\n",
      "Could not determine dtype for column 21, falling back to string\n",
      "Could not determine dtype for column 28, falling back to string\n",
      "Could not determine dtype for column 29, falling back to string\n",
      "Could not determine dtype for column 30, falling back to string\n",
      "Could not determine dtype for column 31, falling back to string\n",
      "Could not determine dtype for column 32, falling back to string\n",
      "Could not determine dtype for column 33, falling back to string\n",
      "Could not determine dtype for column 37, falling back to string\n",
      "Could not determine dtype for column 39, falling back to string\n",
      "Could not determine dtype for column 40, falling back to string\n",
      "Could not determine dtype for column 41, falling back to string\n",
      "Could not determine dtype for column 42, falling back to string\n",
      "Could not determine dtype for column 1, falling back to string\n",
      "Could not determine dtype for column 2, falling back to string\n",
      "Could not determine dtype for column 3, falling back to string\n",
      "Could not determine dtype for column 10, falling back to string\n",
      "Could not determine dtype for column 12, falling back to string\n",
      "Could not determine dtype for column 13, falling back to string\n",
      "Could not determine dtype for column 20, falling back to string\n",
      "Could not determine dtype for column 21, falling back to string\n",
      "Could not determine dtype for column 28, falling back to string\n",
      "Could not determine dtype for column 29, falling back to string\n",
      "Could not determine dtype for column 30, falling back to string\n",
      "Could not determine dtype for column 31, falling back to string\n",
      "Could not determine dtype for column 32, falling back to string\n",
      "Could not determine dtype for column 33, falling back to string\n",
      "Could not determine dtype for column 37, falling back to string\n",
      "Could not determine dtype for column 38, falling back to string\n",
      "Could not determine dtype for column 39, falling back to string\n",
      "Could not determine dtype for column 40, falling back to string\n",
      "Could not determine dtype for column 41, falling back to string\n",
      "Could not determine dtype for column 42, falling back to string\n",
      "Could not determine dtype for column 1, falling back to string\n",
      "Could not determine dtype for column 2, falling back to string\n",
      "Could not determine dtype for column 3, falling back to string\n",
      "Could not determine dtype for column 10, falling back to string\n",
      "Could not determine dtype for column 12, falling back to string\n",
      "Could not determine dtype for column 13, falling back to string\n",
      "Could not determine dtype for column 20, falling back to string\n",
      "Could not determine dtype for column 21, falling back to string\n",
      "Could not determine dtype for column 28, falling back to string\n",
      "Could not determine dtype for column 29, falling back to string\n",
      "Could not determine dtype for column 30, falling back to string\n",
      "Could not determine dtype for column 31, falling back to string\n",
      "Could not determine dtype for column 32, falling back to string\n",
      "Could not determine dtype for column 33, falling back to string\n",
      "Could not determine dtype for column 37, falling back to string\n",
      "Could not determine dtype for column 38, falling back to string\n",
      "Could not determine dtype for column 39, falling back to string\n",
      "Could not determine dtype for column 40, falling back to string\n",
      "Could not determine dtype for column 41, falling back to string\n",
      "Could not determine dtype for column 42, falling back to string\n",
      "Could not determine dtype for column 1, falling back to string\n",
      "Could not determine dtype for column 2, falling back to string\n",
      "Could not determine dtype for column 3, falling back to string\n",
      "Could not determine dtype for column 10, falling back to string\n",
      "Could not determine dtype for column 12, falling back to string\n",
      "Could not determine dtype for column 13, falling back to string\n",
      "Could not determine dtype for column 20, falling back to string\n",
      "Could not determine dtype for column 21, falling back to string\n",
      "Could not determine dtype for column 28, falling back to string\n",
      "Could not determine dtype for column 29, falling back to string\n",
      "Could not determine dtype for column 30, falling back to string\n",
      "Could not determine dtype for column 31, falling back to string\n",
      "Could not determine dtype for column 32, falling back to string\n",
      "Could not determine dtype for column 33, falling back to string\n",
      "Could not determine dtype for column 37, falling back to string\n",
      "Could not determine dtype for column 38, falling back to string\n",
      "Could not determine dtype for column 39, falling back to string\n",
      "Could not determine dtype for column 40, falling back to string\n",
      "Could not determine dtype for column 41, falling back to string\n",
      "Could not determine dtype for column 42, falling back to string\n",
      "Could not determine dtype for column 1, falling back to string\n",
      "Could not determine dtype for column 2, falling back to string\n",
      "Could not determine dtype for column 3, falling back to string\n",
      "Could not determine dtype for column 10, falling back to string\n",
      "Could not determine dtype for column 12, falling back to string\n",
      "Could not determine dtype for column 13, falling back to string\n",
      "Could not determine dtype for column 20, falling back to string\n",
      "Could not determine dtype for column 21, falling back to string\n",
      "Could not determine dtype for column 28, falling back to string\n",
      "Could not determine dtype for column 29, falling back to string\n",
      "Could not determine dtype for column 30, falling back to string\n",
      "Could not determine dtype for column 31, falling back to string\n",
      "Could not determine dtype for column 32, falling back to string\n",
      "Could not determine dtype for column 33, falling back to string\n",
      "Could not determine dtype for column 37, falling back to string\n",
      "Could not determine dtype for column 38, falling back to string\n",
      "Could not determine dtype for column 39, falling back to string\n",
      "Could not determine dtype for column 40, falling back to string\n",
      "Could not determine dtype for column 41, falling back to string\n",
      "Could not determine dtype for column 42, falling back to string\n",
      "Could not determine dtype for column 1, falling back to string\n",
      "Could not determine dtype for column 2, falling back to string\n",
      "Could not determine dtype for column 3, falling back to string\n",
      "Could not determine dtype for column 10, falling back to string\n",
      "Could not determine dtype for column 12, falling back to string\n",
      "Could not determine dtype for column 13, falling back to string\n",
      "Could not determine dtype for column 20, falling back to string\n",
      "Could not determine dtype for column 21, falling back to string\n",
      "Could not determine dtype for column 28, falling back to string\n",
      "Could not determine dtype for column 29, falling back to string\n",
      "Could not determine dtype for column 30, falling back to string\n",
      "Could not determine dtype for column 31, falling back to string\n",
      "Could not determine dtype for column 32, falling back to string\n",
      "Could not determine dtype for column 33, falling back to string\n",
      "Could not determine dtype for column 37, falling back to string\n",
      "Could not determine dtype for column 38, falling back to string\n",
      "Could not determine dtype for column 39, falling back to string\n",
      "Could not determine dtype for column 40, falling back to string\n",
      "Could not determine dtype for column 41, falling back to string\n",
      "Could not determine dtype for column 42, falling back to string\n",
      "Could not determine dtype for column 1, falling back to string\n",
      "Could not determine dtype for column 2, falling back to string\n",
      "Could not determine dtype for column 3, falling back to string\n",
      "Could not determine dtype for column 10, falling back to string\n",
      "Could not determine dtype for column 12, falling back to string\n",
      "Could not determine dtype for column 13, falling back to string\n",
      "Could not determine dtype for column 20, falling back to string\n",
      "Could not determine dtype for column 21, falling back to string\n",
      "Could not determine dtype for column 28, falling back to string\n",
      "Could not determine dtype for column 29, falling back to string\n",
      "Could not determine dtype for column 30, falling back to string\n",
      "Could not determine dtype for column 31, falling back to string\n",
      "Could not determine dtype for column 32, falling back to string\n",
      "Could not determine dtype for column 33, falling back to string\n",
      "Could not determine dtype for column 37, falling back to string\n",
      "Could not determine dtype for column 38, falling back to string\n",
      "Could not determine dtype for column 39, falling back to string\n",
      "Could not determine dtype for column 40, falling back to string\n",
      "Could not determine dtype for column 41, falling back to string\n",
      "Could not determine dtype for column 42, falling back to string\n",
      "Could not determine dtype for column 1, falling back to string\n",
      "Could not determine dtype for column 2, falling back to string\n",
      "Could not determine dtype for column 3, falling back to string\n",
      "Could not determine dtype for column 10, falling back to string\n",
      "Could not determine dtype for column 12, falling back to string\n",
      "Could not determine dtype for column 13, falling back to string\n",
      "Could not determine dtype for column 20, falling back to string\n",
      "Could not determine dtype for column 21, falling back to string\n",
      "Could not determine dtype for column 28, falling back to string\n",
      "Could not determine dtype for column 29, falling back to string\n",
      "Could not determine dtype for column 30, falling back to string\n",
      "Could not determine dtype for column 31, falling back to string\n",
      "Could not determine dtype for column 32, falling back to string\n",
      "Could not determine dtype for column 33, falling back to string\n",
      "Could not determine dtype for column 37, falling back to string\n",
      "Could not determine dtype for column 39, falling back to string\n",
      "Could not determine dtype for column 40, falling back to string\n",
      "Could not determine dtype for column 41, falling back to string\n",
      "Could not determine dtype for column 42, falling back to string\n",
      "Could not determine dtype for column 1, falling back to string\n",
      "Could not determine dtype for column 2, falling back to string\n",
      "Could not determine dtype for column 3, falling back to string\n",
      "Could not determine dtype for column 10, falling back to string\n",
      "Could not determine dtype for column 12, falling back to string\n",
      "Could not determine dtype for column 13, falling back to string\n",
      "Could not determine dtype for column 20, falling back to string\n",
      "Could not determine dtype for column 21, falling back to string\n",
      "Could not determine dtype for column 28, falling back to string\n",
      "Could not determine dtype for column 29, falling back to string\n",
      "Could not determine dtype for column 30, falling back to string\n",
      "Could not determine dtype for column 31, falling back to string\n",
      "Could not determine dtype for column 32, falling back to string\n",
      "Could not determine dtype for column 33, falling back to string\n",
      "Could not determine dtype for column 37, falling back to string\n",
      "Could not determine dtype for column 38, falling back to string\n",
      "Could not determine dtype for column 39, falling back to string\n",
      "Could not determine dtype for column 40, falling back to string\n",
      "Could not determine dtype for column 41, falling back to string\n",
      "Could not determine dtype for column 42, falling back to string\n",
      "Could not determine dtype for column 10, falling back to string\n",
      "Could not determine dtype for column 13, falling back to string\n",
      "Could not determine dtype for column 27, falling back to string\n",
      "Could not determine dtype for column 28, falling back to string\n",
      "Could not determine dtype for column 29, falling back to string\n",
      "Could not determine dtype for column 30, falling back to string\n",
      "Could not determine dtype for column 31, falling back to string\n",
      "Could not determine dtype for column 32, falling back to string\n",
      "Could not determine dtype for column 33, falling back to string\n",
      "Could not determine dtype for column 37, falling back to string\n",
      "Could not determine dtype for column 39, falling back to string\n",
      "Could not determine dtype for column 40, falling back to string\n",
      "Could not determine dtype for column 41, falling back to string\n",
      "Could not determine dtype for column 42, falling back to string\n",
      "Could not determine dtype for column 10, falling back to string\n",
      "Could not determine dtype for column 13, falling back to string\n",
      "Could not determine dtype for column 28, falling back to string\n",
      "Could not determine dtype for column 29, falling back to string\n",
      "Could not determine dtype for column 30, falling back to string\n",
      "Could not determine dtype for column 31, falling back to string\n",
      "Could not determine dtype for column 32, falling back to string\n",
      "Could not determine dtype for column 33, falling back to string\n",
      "Could not determine dtype for column 37, falling back to string\n",
      "Could not determine dtype for column 39, falling back to string\n",
      "Could not determine dtype for column 40, falling back to string\n",
      "Could not determine dtype for column 41, falling back to string\n",
      "Could not determine dtype for column 42, falling back to string\n",
      "C:\\Users\\gibra\\AppData\\Local\\Temp\\ipykernel_13120\\2537434245.py:184: UserWarning: Tight layout not applied. The left and right margins cannot be made large enough to accommodate all Axes decorations.\n",
      "  plt.tight_layout()\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dfSalesMerged = createMergedDataFrameFromExcelMatch(directory= DIRECTORY, keyword= KEYWORDS, columns= SALES_COLUMNS, saveToExcel= False, outputPath= OUTPUT_PATH, outputFileName= OUTPUT_FILE_NAME)\n",
    "\n",
    "dfSalesMerged = cleanDateColumn(dfSalesMerged, DATE)\n",
    "\n",
    "generateSalesEDA(dfSalesMerged, \"../output/salesEDAReport.pdf\", days=90)\n",
    "\n",
    "#saveDataFrameToExcel(dfSalesMerged, OUTPUT_PATH, OUTPUT_FILE_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395a24fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
